clc, clear, close all;

% costs = []
% 
% parameters = initialize_parameters_deep(layers_dims)
%     
% for i = 1:num_iterations
%     % Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.
%     AL, caches = L_model_forward(X, parameters)
%         
%     % Compute cost.
%     cost = compute_cost(AL, Y)
%     
%     % Backward propagation.
%     grads = L_model_backward(AL, Y, caches)
%  
%     % Update parameters.
%     parameters = update_parameters(parameters, grads, learning_rate)
%                 
%     % Print the cost every 100 training example
%     if print_cost && mod(i, 100) == 0
%         sprintf("Cost after iteration %i: %f", i, cost)
%         costs.append(cost)
%     end
%             
%     % plot the cost
%     plot(np.squeeze(costs))
%     ylabel('cost')
%     xlabel('iterations (per tens)')
%     title("Learning rate =" + str(learning_rate))
% end



